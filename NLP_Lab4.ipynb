{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRjA95LkeWIu"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from matplotlib import pylab\n",
        "from collections import Counter\n",
        "import csv\n",
        "\n",
        "# Seq2Seq Items\n",
        "import tensorflow.contrib.seq2seq as seq2seq\n",
        "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
        "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
        "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
        "from tensorflow.python.layers.core import Dense\n",
        "vocab_size= 50000\n",
        "num_units = 128\n",
        "input_size = 128\n",
        "batch_size = 16\n",
        "source_sequence_length=40\n",
        "target_sequence_length=60\n",
        "decoder_type = 'basic' # could be basic or attention\n",
        "sentences_to_read = 50000\n",
        "src_dictionary = dict()\n",
        "with open('vocab.50K.de.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        #we are discarding last char as it is new line char\n",
        "        src_dictionary[line[:-1]] = len(src_dictionary)\n",
        "\n",
        "src_reverse_dictionary =\n",
        "dict(zip(src_dictionary.values(),src_dictionary.keys()))\n",
        "\n",
        "print('Source')\n",
        "print('\\t',list(src_dictionary.items())[:10])\n",
        "print('\\t',list(src_reverse_dictionary.items())[:10])\n",
        "print('\\t','Vocabulary size: ', len(src_dictionary))\n",
        "\n",
        "tgt_dictionary = dict()\n",
        "with open('vocab.50K.en.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        #we are discarding last char as it is new line char\n",
        "        tgt_dictionary[line[:-1]] = len(tgt_dictionary)\n",
        "\n",
        "tgt_reverse_dictionary =\n",
        "dict(zip(tgt_dictionary.values(),tgt_dictionary.keys()))\n",
        "\n",
        "print('Target')\n",
        "print('\\t',list(tgt_dictionary.items())[:10])\n",
        "print('\\t',list(tgt_reverse_dictionary.items())[:10])\n",
        "print('\\t','Vocabulary size: ', len(tgt_dictionary))\n",
        "\n",
        "source_sent = []\n",
        "target_sent = []\n",
        "\n",
        "test_source_sent = []\n",
        "test_target_sent = []\n",
        "\n",
        "\n",
        "with open('train.de', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        # discarding first 20 translations as there was some\n",
        "\n",
        "    # english to english translations found in the first few. which\n",
        "are wrong\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        source_sent.append(line)\n",
        "        if len(source_sent)>=sentences_to_read:\n",
        "            break\n",
        "\n",
        "\n",
        "with open('train.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        if l_i<50:\n",
        "            continue\n",
        "\n",
        "        target_sent.append(line)\n",
        "        if len(target_sent)>=sentences_to_read:\n",
        "            break\n",
        "\n",
        "\n",
        "assert len(source_sent)==len(target_sent),'Source: %d, Target:\n",
        "%d'%(len(source_sent),len(target_sent))\n",
        "\n",
        "print('Sample translations (%d)'%len(source_sent))\n",
        "for i in range(0,sentences_to_read,10000):\n",
        "    print('(',i,') DE: ', source_sent[i])\n",
        "    print('(',i,') EN: ', target_sent[i])\n",
        "\n",
        "def split_to_tokens(sent,is_source):\n",
        "    #sent = sent.replace('-',' ')\n",
        "    sent = sent.replace(',',' ,')\n",
        "    sent = sent.replace('.',' .')\n",
        "    sent = sent.replace('\\n',' ')\n",
        "\n",
        "    sent_toks = sent.split(' ')\n",
        "    for t_i, tok in enumerate(sent_toks):\n",
        "        if is_source:\n",
        "            if tok not in src_dictionary.keys():\n",
        "                sent_toks[t_i] = '<unk>'\n",
        "        else:\n",
        "            if tok not in tgt_dictionary.keys():\n",
        "                sent_toks[t_i] = '<unk>'\n",
        "    return sent_toks\n",
        "\n",
        "# Let us first look at some statistics of the sentences\n",
        "source_len = []\n",
        "source_mean, source_std = 0,0\n",
        "for sent in source_sent:\n",
        "    source_len.append(len(split_to_tokens(sent,True)))\n",
        "\n",
        "print('(Source) Sentence mean length: ', np.mean(source_len))\n",
        "print('(Source) Sentence stddev length: ', np.std(source_len))\n",
        "\n",
        "target_len = []\n",
        "target_mean, target_std = 0,0\n",
        "for sent in target_sent:\n",
        "    target_len.append(len(split_to_tokens(sent,False)))\n",
        "\n",
        "print('(Target) Sentence mean length: ', np.mean(target_len))\n",
        "print('(Target) Sentence stddev length: ', np.std(target_len))\n",
        "train_inputs = []\n",
        "train_outputs = []\n",
        "train_inp_lengths = []\n",
        "train_out_lengths = []\n",
        "\n",
        "max_tgt_sent_lengths = 0\n",
        "\n",
        "src_max_sent_length = 41\n",
        "tgt_max_sent_length = 61\n",
        "for s_i, (src_sent, tgt_sent) in enumerate(zip(source_sent,target_sent)):\n",
        "\n",
        "    src_sent_tokens = split_to_tokens(src_sent,True)\n",
        "    tgt_sent_tokens = split_to_tokens(tgt_sent,False)\n",
        "\n",
        "    num_src_sent = []\n",
        "    for tok in src_sent_tokens:\n",
        "        num_src_sent.append(src_dictionary[tok])\n",
        "\n",
        "    num_src_set = num_src_sent[::-1] # we reverse the source sentence.\n",
        "This improves performance\n",
        "    num_src_sent.insert(0,src_dictionary['<s>'])\n",
        "    train_inp_lengths.append(min(len(num_src_sent)+1,src_max_sent_length))\n",
        "\n",
        "    # append until the sentence reaches max length\n",
        "    if len(num_src_sent)<src_max_sent_length:\n",
        "        num_src_sent.extend([src_dictionary['</s>'] for _ in\n",
        "range(src_max_sent_length - len(num_src_sent))])\n",
        "    # if more than max length, truncate the sentence\n",
        "    elif len(num_src_sent)>src_max_sent_length:\n",
        "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
        "    assert len(num_src_sent)==src_max_sent_length,len(num_src_sent)\n",
        "\n",
        "    train_inputs.append(num_src_sent)\n",
        "\n",
        "    num_tgt_sent = [tgt_dictionary['</s>']]\n",
        "    for tok in tgt_sent_tokens:\n",
        "        num_tgt_sent.append(tgt_dictionary[tok])\n",
        "\n",
        "    train_out_lengths.append(min(len(num_tgt_sent)+1,tgt_max_sent_length))\n",
        "\n",
        "    if len(num_tgt_sent)<tgt_max_sent_length:\n",
        "        num_tgt_sent.extend([tgt_dictionary['</s>'] for _ in\n",
        "range(tgt_max_sent_length - len(num_tgt_sent))])\n",
        "    elif len(num_tgt_sent)>tgt_max_sent_length:\n",
        "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
        "\n",
        "    train_outputs.append(num_tgt_sent)\n",
        "    assert len(train_outputs[s_i])==tgt_max_sent_length, 'Sent length\n",
        "needs to be 60, but is %d'%len(binned_outputs[s_i])\n",
        "\n",
        "assert len(train_inputs)  == len(source_sent),\\\n",
        "        'Size of total bin elements: %d, Total sentences: %d'\\\n",
        "                %(len(train_inputs),len(source_sent))\n",
        "\n",
        "print('Max sent lengths: ', max_tgt_sent_lengths)\n",
        "\n",
        "\n",
        "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
        "train_outputs = np.array(train_outputs, dtype=np.int32)\n",
        "train_inp_lengths = np.array(train_inp_lengths, dtype=np.int32)\n",
        "train_out_lengths = np.array(train_out_lengths, dtype=np.int32)\n",
        "print('Samples from bin')\n",
        "print('\\t',[src_reverse_dictionary[w]  for w in\n",
        "train_inputs[0,:].tolist()])\n",
        "print('\\t',[tgt_reverse_dictionary[w]  for w in\n",
        "train_outputs[0,:].tolist()])\n",
        "print('\\t',[src_reverse_dictionary[w]  for w in\n",
        "train_inputs[10,:].tolist()])\n",
        "print('\\t',[tgt_reverse_dictionary[w]  for w in\n",
        "train_outputs[10,:].tolist()])\n",
        "print()\n",
        "print('\\tSentences ',train_inputs.shape[0])\n",
        "input_size = 128\n",
        "\n",
        "class DataGeneratorMT(object):\n",
        "\n",
        "    def __init__(self,batch_size,num_unroll,is_source):\n",
        "        self._batch_size = batch_size\n",
        "        self._num_unroll = num_unroll\n",
        "        self._cursor = [0 for offset in range(self._batch_size)]\n",
        "\n",
        "\n",
        "        self._src_word_embeddings = np.load('de-embeddings.npy')\n",
        "\n",
        "        self._tgt_word_embeddings = np.load('en-embeddings.npy')\n",
        "\n",
        "        self._sent_ids = None\n",
        "\n",
        "        self._is_source = is_source\n",
        "\n",
        "\n",
        "    def next_batch(self, sent_ids, first_set):\n",
        "\n",
        "        if self._is_source:\n",
        "            max_sent_length = src_max_sent_length\n",
        "        else:\n",
        "            max_sent_length = tgt_max_sent_length\n",
        "        batch_labels_ind = []\n",
        "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
        "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
        "\n",
        "        for b in range(self._batch_size):\n",
        "\n",
        "            sent_id = sent_ids[b]\n",
        "\n",
        "            if self._is_source:\n",
        "                sent_text = train_inputs[sent_id]\n",
        "\n",
        "                batch_data[b] = sent_text[self._cursor[b]]\n",
        "                batch_labels[b]=sent_text[self._cursor[b]+1]\n",
        "\n",
        "            else:\n",
        "                sent_text = train_outputs[sent_id]\n",
        "\n",
        "                batch_data[b] = sent_text[self._cursor[b]]\n",
        "                batch_labels[b] = sent_text[self._cursor[b]+1]\n",
        "\n",
        "            self._cursor[b] = (self._cursor[b]+1)%(max_sent_length-1)\n",
        "\n",
        "        return batch_data,batch_labels\n",
        "\n",
        "    def unroll_batches(self,sent_ids):\n",
        "\n",
        "        if sent_ids is not None:\n",
        "\n",
        "            self._sent_ids = sent_ids\n",
        "\n",
        "            self._cursor = [0 for _ in range(self._batch_size)]\n",
        "\n",
        "        unroll_data,unroll_labels = [],[]\n",
        "        inp_lengths = None\n",
        "        for ui in range(self._num_unroll):\n",
        "\n",
        "            data, labels = self.next_batch(self._sent_ids, False)\n",
        "\n",
        "            unroll_data.append(data)\n",
        "            unroll_labels.append(labels)\n",
        "            inp_lengths = train_inp_lengths[sent_ids]\n",
        "        return unroll_data, unroll_labels, self._sent_ids, inp_lengths\n",
        "\n",
        "    def reset_indices(self):\n",
        "        self._cursor = [0 for offset in range(self._batch_size)]\n",
        "\n",
        "# Running a tiny set to see if the implementation correct\n",
        "dg = DataGeneratorMT(batch_size=5,num_unroll=40,is_source=True)\n",
        "u_data, u_labels, _, _ = dg.unroll_batches([0,1,2,3,4])\n",
        "\n",
        "print('Source data')\n",
        "for _, lbl in zip(u_data,u_labels):\n",
        "    print([src_reverse_dictionary[w] for w in lbl.tolist()])\n",
        "\n",
        "\n",
        "# Running a tiny set to see if the implementation correct\n",
        "dg = DataGeneratorMT(batch_size=5,num_unroll=60,is_source=False)\n",
        "u_data, u_labels, _, _ = dg.unroll_batches([0,2,3,4,5])\n",
        "print('\\nTarget data batch (first time)')\n",
        "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
        "    #if d_i>5 and d_i < 35:\n",
        "    #    continue\n",
        "\n",
        "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])\n",
        "\n",
        "print('\\nTarget data batch (non-first time)')\n",
        "u_data, u_labels, _, _ = dg.unroll_batches(None)\n",
        "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
        "\n",
        "    #if d_i>5 and d_i < 35:\n",
        "    #    continue\n",
        "\n",
        "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "enc_train_inputs = []\n",
        "dec_train_inputs = []\n",
        "\n",
        "# Need to use pre-trained word embeddings\n",
        "encoder_emb_layer = tf.convert_to_tensor(np.load('de-embeddings.npy'))\n",
        "decoder_emb_layer = tf.convert_to_tensor(np.load('en-embeddings.npy'))\n",
        "\n",
        "# Defining unrolled training inputs\n",
        "for ui in range(source_sequence_length):\n",
        "    enc_train_inputs.append(tf.placeholder(tf.int32,\n",
        "shape=[batch_size],name='enc_train_inputs_%d'%ui))\n",
        "\n",
        "dec_train_labels=[]\n",
        "dec_label_masks = []\n",
        "for ui in range(target_sequence_length):\n",
        "    dec_train_inputs.append(tf.placeholder(tf.int32,\n",
        "shape=[batch_size],name='dec_train_inputs_%d'%ui))\n",
        "    dec_train_labels.append(tf.placeholder(tf.int32,\n",
        "shape=[batch_size],name='dec-train_outputs_%d'%ui))\n",
        "    dec_label_masks.append(tf.placeholder(tf.float32,\n",
        "shape=[batch_size],name='dec-label_masks_%d'%ui))\n",
        "\n",
        "encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, src) for src\n",
        "in enc_train_inputs]\n",
        "encoder_emb_inp = tf.stack(encoder_emb_inp)\n",
        "\n",
        "decoder_emb_inp = [tf.nn.embedding_lookup(decoder_emb_layer, src) for src\n",
        "in dec_train_inputs]\n",
        "decoder_emb_inp = tf.stack(decoder_emb_inp)\n",
        "\n",
        "enc_train_inp_lengths = tf.placeholder(tf.int32,\n",
        "shape=[batch_size],name='train_input_lengths')\n",
        "dec_train_inp_lengths = tf.placeholder(tf.int32,\n",
        "shape=[batch_size],name='train_output_lengths')\n",
        "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
        "\n",
        "initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
        "    encoder_cell, encoder_emb_inp, initial_state=initial_state,\n",
        "    sequence_length=enc_train_inp_lengths,\n",
        "    time_major=True, swap_memory=True)\n",
        "# Build RNN cell\n",
        "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
        "\n",
        "projection_layer = Dense(units=vocab_size, use_bias=True)\n",
        "\n",
        "# Helper\n",
        "helper = tf.contrib.seq2seq.TrainingHelper(\n",
        "    decoder_emb_inp, [tgt_max_sent_length-1 for _ in range(batch_size)],\n",
        "time_major=True)\n",
        "\n",
        "# Decoder\n",
        "if decoder_type == 'basic':\n",
        "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "        decoder_cell, helper, encoder_state,\n",
        "        output_layer=projection_layer)\n",
        "\n",
        "elif decoder_type == 'attention':\n",
        "    decoder = tf.contrib.seq2seq.BahdanauAttention(\n",
        "        decoder_cell, helper, encoder_state,\n",
        "        output_layer=projection_layer)\n",
        "\n",
        "# Dynamic decoding\n",
        "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "    decoder, output_time_major=True,\n",
        "    swap_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "logits = outputs.rnn_output\n",
        "\n",
        "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "    labels=dec_train_labels, logits=logits)\n",
        "loss = (tf.reduce_sum(crossent*tf.stack(dec_label_masks)) /\n",
        "(batch_size*target_sequence_length))\n",
        "\n",
        "train_prediction = outputs.sample_id\n",
        "print('Defining Optimizer')\n",
        "# Adam Optimizer. And gradient clipping.\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "inc_gstep = tf.assign(global_step,global_step + 1)\n",
        "learning_rate = tf.train.exponential_decay(\n",
        "    0.01, global_step, decay_steps=10, decay_rate=0.9, staircase=True)\n",
        "\n",
        "with tf.variable_scope('Adam'):\n",
        "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "adam_gradients, v = zip(*adam_optimizer.compute_gradients(loss))\n",
        "adam_gradients, _ = tf.clip_by_global_norm(adam_gradients, 25.0)\n",
        "adam_optimize = adam_optimizer.apply_gradients(zip(adam_gradients, v))\n",
        "\n",
        "with tf.variable_scope('SGD'):\n",
        "    sgd_optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "\n",
        "sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))\n",
        "sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 25.0)\n",
        "sgd_optimize = sgd_optimizer.apply_gradients(zip(sgd_gradients, v))\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "\n",
        "if not os.path.exists('logs'):\n",
        "    os.mkdir('logs')\n",
        "log_dir = 'logs'\n",
        "\n",
        "bleu_scores_over_time = []\n",
        "loss_over_time = []\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "src_word_embeddings = np.load('de-embeddings.npy')\n",
        "tgt_word_embeddings = np.load('en-embeddings.npy')\n",
        "\n",
        "# Defining data generators\n",
        "enc_data_generator =\n",
        "DataGeneratorMT(batch_size=batch_size,num_unroll=source_sequence_length,is\n",
        " _source=True)\n",
        "dec_data_generator =\n",
        "DataGeneratorMT(batch_size=batch_size,num_unroll=target_sequence_length,is\n",
        " _source=False)\n",
        "\n",
        "num_steps = 10001\n",
        "avg_loss = 0\n",
        "\n",
        "bleu_labels, bleu_preds = [],[]\n",
        "\n",
        "print('Started Training')\n",
        "\n",
        "for step in range(num_steps):\n",
        "\n",
        "    # input_sizes for each bin: [40]\n",
        "    # output_sizes for each bin: [60]\n",
        "    print('.',end='')\n",
        "    if (step+1)%100==0:\n",
        "        print('')\n",
        "\n",
        "    sent_ids =\n",
        "np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
        "    # ====================== ENCODER DATA COLLECTION\n",
        "================================================\n",
        "\n",
        "    eu_data, eu_labels, _, eu_lengths =\n",
        "enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
        "\n",
        "    feed_dict = {}\n",
        "    feed_dict[enc_train_inp_lengths] = eu_lengths\n",
        "    for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):\n",
        "        feed_dict[enc_train_inputs[ui]] = dat\n",
        "\n",
        "    # ====================== DECODER DATA COLLECITON\n",
        "===========================\n",
        "    # First step we change the ids in a batch\n",
        "    du_data, du_labels, _, du_lengths =\n",
        "dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
        "\n",
        "    feed_dict[dec_train_inp_lengths] = du_lengths\n",
        "    for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):\n",
        "        feed_dict[dec_train_inputs[ui]] = dat\n",
        "        feed_dict[dec_train_labels[ui]] = lbl\n",
        "        feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in\n",
        "range(batch_size)])<du_lengths).astype(np.int32)\n",
        "\n",
        "    # ======================= OPTIMIZATION ==========================\n",
        "    if step < 10000:\n",
        "        _,l,tr_pred = sess.run([adam_optimize,loss,train_prediction],\n",
        "feed_dict=feed_dict)\n",
        "    else:\n",
        "        _,l,tr_pred = sess.run([sgd_optimize,loss,train_prediction],\n",
        "feed_dict=feed_dict)\n",
        "    tr_pred = tr_pred.flatten()\n",
        "\n",
        "\n",
        "    if (step+1)%250==0:\n",
        "\n",
        "        print('Step ',step+1)\n",
        "\n",
        "        print_str = 'Actual: '\n",
        "        for w in np.concatenate(du_labels,axis=0)[::batch_size].tolist():\n",
        "            print_str += tgt_reverse_dictionary[w] + ' '\n",
        "            if tgt_reverse_dictionary[w] == '</s>':\n",
        "                break\n",
        "\n",
        "        print(print_str)\n",
        "        print()\n",
        "\n",
        "        print_str = 'Predicted: '\n",
        "        for w in tr_pred[::batch_size].tolist():\n",
        "            print_str += tgt_reverse_dictionary[w] + ' '\n",
        "            if tgt_reverse_dictionary[w] == '</s>':\n",
        "                break\n",
        "        print(print_str)\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "        rand_idx = np.random.randint(low=1,high=batch_size)\n",
        "        print_str = 'Actual: '\n",
        "        for w in\n",
        "np.concatenate(du_labels,axis=0)[rand_idx::batch_size].tolist():\n",
        "            print_str += tgt_reverse_dictionary[w] + ' '\n",
        "            if tgt_reverse_dictionary[w] == '</s>':\n",
        "                break\n",
        "        print(print_str)\n",
        "\n",
        "\n",
        "        print()\n",
        "        print_str = 'Predicted: '\n",
        "        for w in tr_pred[rand_idx::batch_size].tolist():\n",
        "            print_str += tgt_reverse_dictionary[w] + ' '\n",
        "            if tgt_reverse_dictionary[w] == '</s>':\n",
        "                break\n",
        "        print(print_str)\n",
        "        print()\n",
        "\n",
        "    avg_loss += l\n",
        "\n",
        "    #sess.run(reset_train_state) # resetting hidden state for each batch\n",
        "\n",
        "    if (step+1)%500==0:\n",
        "        print('============= Step ', str(step+1), ' =============')\n",
        "        print('\\t Loss: ',avg_loss/500.0)\n",
        "\n",
        "        loss_over_time.append(avg_loss/500.0)\n",
        "\n",
        "        avg_loss = 0.0\n",
        "        sess.run(inc_gstep)\n",
        "\n",
        ""
      ]
    }
  ]
}