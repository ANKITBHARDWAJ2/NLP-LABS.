{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDpw3fmJeGFI"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # Data manipulation\n",
        "import re  # Regular expressions for text cleaning\n",
        "import gensim  # For LDA and word embeddings\n",
        "import nltk  # Natural Language Toolkit\n",
        "from nltk.corpus import stopwords  # Common stopwords (e.g., \"the\", \"is\")\n",
        "from nltk.stem import WordNetLemmatizer  # Reduce words to base form (e.g., \"running\" → \"run\")\n",
        "from string import punctuation  # Punctuation marks (e.g., \".\", \",\")\n",
        "from gensim.corpora import Dictionary  # Create a word-to-id mapping for LDA\n",
        "from nltk.tokenize import word_tokenize  # Split text into words\n",
        "from gensim.models.ldamodel import LdaModel, CoherenceModel  # LDA model and evaluation\n",
        "import pyLDAvis  # Interactive topic visualization\n",
        "import pyLDAvis.gensim  # Gensim integration for pyLDAvis\n",
        "import matplotlib.pyplot as plt  # Plotting (not used here but often helpful)\n",
        "%matplotlib inline  # Display plots in Jupyter Notebook\n",
        "\n",
        "# Load the dataset (20 Newsgroups JSON format)\n",
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(\"Dataset Preview:\")\n",
        "df.head()  # Show first 5 rows\n",
        "\n",
        "# Text Preprocessing Functions\n",
        "\n",
        "def removing_email(text):\n",
        "    \"\"\"Remove email addresses using regex.\"\"\"\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', ' ', text)  # Matches patterns like user@domain.com\n",
        "    return text\n",
        "\n",
        "def only_words(text):\n",
        "    \"\"\"Keep only alphanumeric words and spaces (removes special characters).\"\"\"\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Replace non-alphabets with space\n",
        "    return text\n",
        "\n",
        "# Define stopwords (common words to exclude)\n",
        "stop_words = (\n",
        "    list(set(stopwords.words('english'))) +  # Standard English stopwords\n",
        "    list(punctuation) +  # Punctuation marks\n",
        "    ['\\n', '----', '---\\n\\n\\n\\n\\n']  # Additional noise (e.g., line breaks)\n",
        ")\n",
        "\n",
        "lem = WordNetLemmatizer()  # Initialize lemmatizer\n",
        "\n",
        "def cleaning(text):\n",
        "    \"\"\"Full text cleaning pipeline:\n",
        "    1. Lowercase\n",
        "    2. Tokenize (split into words)\n",
        "    3. Remove stopwords\n",
        "    4. Filter short words (<3 chars)\n",
        "    5. Lemmatize verbs (e.g., \"running\" → \"run\")\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    words = word_tokenize(text)\n",
        "    words = [w for w in words if w not in stop_words]  # Remove stopwords\n",
        "    words = [w for w in words if len(w) >= 3]  # Keep meaningful words\n",
        "    lemma = [lem.lemmatize(w, 'v') for w in words]  # Lemmatize verbs\n",
        "    return lemma\n",
        "\n",
        "# Apply preprocessing pipeline\n",
        "df['without email'] = df['content'].apply(removing_email)\n",
        "df['only words'] = df['without email'].apply(only_words)\n",
        "df['clean content'] = df['only words'].apply(cleaning)\n",
        "print(\"\\nProcessed Data Preview:\")\n",
        "df.head()\n",
        "\n",
        "# Prepare data for LDA\n",
        "clean_doc = list(df['clean content'].values)  # List of tokenized documents\n",
        "\n",
        "\"\"\"\n",
        "Gensim Dictionary:\n",
        "- Maps each word to a unique ID\n",
        "- Filters extremes (optional, but improves model quality)\n",
        "\"\"\"\n",
        "dictionary = Dictionary(clean_doc)\n",
        "# dictionary.filter_extremes(no_below=5, no_above=0.5)  # Optional: Remove rare/common words\n",
        "\n",
        "\"\"\"\n",
        "Corpus Creation:\n",
        "- Converts documents into Bag-of-Words (BoW) format\n",
        "- Each document: List of (word_id, frequency) tuples\n",
        "\"\"\"\n",
        "corpus = [dictionary.doc2bow(doc) for doc in clean_doc]\n",
        "\n",
        "\"\"\"\n",
        "LDA Model Training:\n",
        "- num_topics=5: Number of latent topics to extract\n",
        "- random_state=42: Reproducibility\n",
        "- passes=50: Number of full corpus passes (more passes → better convergence)\n",
        "- chunksize=100: Number of docs processed per training chunk\n",
        "\"\"\"\n",
        "ldamodel = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=5,\n",
        "    random_state=42,\n",
        "    update_every=1,\n",
        "    passes=50,\n",
        "    chunksize=100,\n",
        "    alpha='auto',  # Let model learn topic distributions\n",
        "    eta='auto'  # Let model learn word distributions\n",
        ")\n",
        "\n",
        "# Display Topics (Each topic: Top 10 words + weights)\n",
        "print(\"\\nDiscovered Topics:\")\n",
        "print(ldamodel.print_topics())\n",
        "\n",
        "\"\"\"\n",
        "Model Evaluation:\n",
        "1. Perplexity: Lower = Better (Measures how well model predicts unseen data)\n",
        "   - log_perplexity() returns the bound, not true perplexity\n",
        "\"\"\"\n",
        "print(\"\\nLog Perplexity:\", ldamodel.log_perplexity(corpus))\n",
        "\n",
        "\"\"\"\n",
        "2. Coherence Scores:\n",
        "- c_v: Higher = Better (0-1, measures topic interpretability)\n",
        "- u_mass: Closer to 0 = Better (can be negative)\n",
        "\"\"\"\n",
        "coherence_cv = CoherenceModel(\n",
        "    model=ldamodel,\n",
        "    texts=clean_doc,\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        ")\n",
        "print(\"\\nCoherence (c_v):\", coherence_cv.get_coherence())\n",
        "\n",
        "coherence_umass = CoherenceModel(\n",
        "    model=ldamodel,\n",
        "    texts=clean_doc,\n",
        "    dictionary=dictionary,\n",
        "    coherence='u_mass'\n",
        ")\n",
        "print(\"Coherence (u_mass):\", coherence_umass.get_coherence())\n",
        "\n",
        "\"\"\"\n",
        "Interactive Visualization with pyLDAvis:\n",
        "- λ (lambda) slider adjusts term relevance\n",
        "- Bubble size = Topic prevalence\n",
        "- Distance between topics ≈ dissimilarity\n",
        "\"\"\"\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
        "vis"
      ]
    }
  ]
}