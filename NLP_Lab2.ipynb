{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NICiKnuNc8l6"
      },
      "outputs": [],
      "source": [
        "# Import gensim's model downloader utility which provides access to pre-trained word embedding models\n",
        "import gensim.downloader\n",
        "\n",
        "# Suppress warnings to keep the output clean (not recommended for development/debugging)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\"\"\"\n",
        "Gensim Model Downloader Explanation:\n",
        "- gensim.downloader provides access to various pre-trained word embedding models\n",
        "- These models are trained on massive corpora and capture semantic relationships between words\n",
        "- Available models include Word2Vec (Google News), GloVe (Wikipedia), FastText, etc.\n",
        "\"\"\"\n",
        "# List all available pre-trained models in gensim's repository\n",
        "print(\"Available pre-trained models in gensim:\")\n",
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        "\n",
        "\"\"\"\n",
        "Word2Vec Model Loading:\n",
        "- 'word2vec-google-news-300' is a Word2Vec model trained on Google News corpus\n",
        "- Contains 300-dimensional vectors for 3 million words/phrases\n",
        "- Takes significant memory to load (~1.5GB)\n",
        "- The vectors capture semantic and syntactic word relationships\n",
        "\"\"\"\n",
        "print(\"\\nLoading word2vec-google-news-300 model (this may take several minutes)...\")\n",
        "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
        "\n",
        "\"\"\"\n",
        "Most Similar Words Demonstration:\n",
        "- The most_similar() method finds words with vectors most similar to the target word\n",
        "- Uses cosine similarity between word vectors\n",
        "- Shows how the model captures semantic relationships\n",
        "\"\"\"\n",
        "print(\"\\nWords most similar to 'technology':\")\n",
        "print(word2vec.most_similar('technology'))\n",
        "\"\"\"\n",
        "Expected Output Insight:\n",
        "- Returns related terms like 'technologies', 'science', 'engineering'\n",
        "- Shows the model understands technology-related concepts\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nWords most similar to 'Science':\")\n",
        "print(word2vec.most_similar('Science'))\n",
        "\"\"\"\n",
        "Capitalization Note:\n",
        "- The model is case-insensitive for English words\n",
        "- 'Science' and 'science' will yield similar results\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nWords most similar to 'arts':\")\n",
        "print(word2vec.most_similar('arts'))\n",
        "\"\"\"\n",
        "Semantic Field Example:\n",
        "- Should return related terms from arts/culture domain\n",
        "- Demonstrates the model's understanding of different knowledge domains\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Word Similarity Measurement:\n",
        "- similarity() calculates cosine similarity between two word vectors\n",
        "- Returns value between -1 (opposite) and 1 (identical)\n",
        "- For related words, typically between 0.5-0.8\n",
        "\"\"\"\n",
        "print(\"\\nSimilarity between 'hot' and 'cold':\")\n",
        "print(word2vec.similarity('hot', 'cold'))\n",
        "\"\"\"\n",
        "Interesting Observation:\n",
        "- While antonyms, they have high similarity because:\n",
        "  1. Both are temperature-related terms\n",
        "  2. Often appear in similar contexts\n",
        "  3. Share many syntactic relationships\n",
        "- Shows word vectors capture linguistic relationships beyond simple synonymy\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Additional Key Concepts:\n",
        "\n",
        "1. Word Embedding Properties:\n",
        "- Similar words cluster together in vector space\n",
        "- Vector arithmetic captures relationships (king - man + woman â‰ˆ queen)\n",
        "- The 300 dimensions encode various linguistic features\n",
        "\n",
        "2. Model Limitations:\n",
        "- Fixed vocabulary (out-of-vocabulary words won't work)\n",
        "- Context-insensitive (same vector for all word senses)\n",
        "- Trained on news data - biases present in source material\n",
        "\n",
        "3. Practical Applications:\n",
        "- Semantic search\n",
        "- Document clustering\n",
        "- As features for machine learning models\n",
        "- Analogical reasoning\n",
        "\n",
        "4. Alternative Methods:\n",
        "- Contextual embeddings (BERT, ELMo) for polysemous words\n",
        "- Domain-specific embeddings for specialized vocabularies\n",
        "\"\"\""
      ]
    }
  ]
}